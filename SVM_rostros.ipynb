{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de rostros utilizando máquinas de vectores de soporte y análisis en componentes principales.\n",
    "\n",
    "En esta libreta vamos a hacer el clásico algritmo conocido como *eigenfaces* con el fin de ilustrar el uso de las máquinas de vector de soporte y los componentes principales. Para darle más énfasis a esto, no vamos a utilizar los métodos de análisis y optimización que ya existen en Scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gzip import GzipFile  #  Para abrir el archivo comprimido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Carga imágenes y nombres a partir de los archivos\n",
    "\n",
    "Para este problema, se utilizaron ejemplos del conjunto de imagenes\n",
    "[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz), \n",
    "el cual tiene un tamaño de 233MB.\n",
    "\n",
    "Los datos preprocesados se obtuvieron de la librería de bases de datos de\n",
    "prueba de [scikit.learn](http://sourceforge.net/projects/scikit-learn/files/data/), \n",
    "pero los presentamos en esta libreta por separado, con el fn de mostrar un poco el proceso\n",
    "completo paso a paso.\n",
    "  \n",
    "Para preprocesar las imagenes, se utilizó [openCV](http://opencv.willowgarage.com/wiki/Welcome).\n",
    "En cada imagen:\n",
    "\n",
    "1. Se utilizó *OpenCV* para localizar y extraer el rostro\n",
    "\n",
    "2. Se recortó y escaló a una resolución de 64x64 pixeles\n",
    "\n",
    "3. Se convirtió a escala de grises\n",
    "\n",
    "4. Se transformó a un arreglo tipo *numpy*\n",
    "\n",
    "5. Se colocaron una imagen sobre otra \n",
    "\n",
    "6. Se guardo como un archivo de numpy (usando np.save)\n",
    "\n",
    "7. Se comprimió el archivo usando el método gzip (sin comprimir son 250 Mb).\n",
    "\n",
    "El archivo es ``faces.npy.gz``\n",
    "\n",
    "Los nombres de las imágenes (donde se puede extraer el nombre\n",
    "de la persona que aparece) se guardó en el archivo texto ``face_filenames.txt``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caras = np.load(GzipFile(\"faces.npy.gz\"))\n",
    "\n",
    "nombre_caras = [l.strip().rsplit('_',1)[0]\n",
    "                for l in open(\"face_filenames.txt\").readlines()]\n",
    "\n",
    "\n",
    "# A cada cara se le quita el brillo restandole el\n",
    "# valor medio de brillo de cada una.\n",
    "caras = caras - caras.mean(axis=1).reshape(caras.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ejemplo para ver las imagenes\n",
    "\n",
    "ej = np.random.randint(caras.shape[0])\n",
    "imagen_ejemplo = np.reshape(caras[ej,:].copy(), (64,64))\n",
    "plt.imshow(imagen_ejemplo, cmap=plt.cm.gray)\n",
    "plt.title(nombre_caras[ej])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Acondicionamiento de los datos de objetivos T \n",
    "\n",
    "\n",
    "Una vez que se tiene bien establecido el objetivo, entonces\n",
    "hay que generar el vector Y de salidas o *targets*.\n",
    "\n",
    "Para esto vamos a utilizar varias operaciones de ``numpy`` que\n",
    "no habíamos utilizado antes.\n",
    "\n",
    "- ``np.unique(A)`` devuelve un vector con los valores diferentes de A.\n",
    "\n",
    "- ``np.searchsorted(A, V)`` regresa un vector de indices donde se encuentra cada elemento de A en V.\n",
    "\n",
    "- ``np.bincount(X)`` cuenta el número de ocurrencias \n",
    "\n",
    "- ``np.in1d(A,B)`` encuentra los indices donde en A hay un elemento que se encuentra el B. Lo devuelve como\n",
    "  False o True, por lo que se puede utilizar a lo largo de los renglones sin problemas.\n",
    "\n",
    "Como se tienen demasiadas clases (una por cada nombre), lo\n",
    "que vamos a hacer es reducir la complejidad del problema a\n",
    "solamente clasificar los 5 rostros mas repetidos en los datos.\n",
    "Así tenemos que encontrar las 5 clases con mayor frecuencia en\n",
    "los datos y a partir de ahi obtener las matrices X y T para\n",
    "aprendizaje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Genera el vector de targets\n",
    "nombre_caras_array = np.array(nombre_caras)\n",
    "caras_diferentes = np.unique(nombre_caras_array)\n",
    "Y_completa = np.searchsorted(caras_diferentes, nombre_caras_array)\n",
    "\n",
    "# Encuentra las clases (en números enteros) más repetidas en T\n",
    "clases_retenidas = np.argsort(np.bincount(Y_completa))[-5:]\n",
    "\n",
    "\n",
    "# Obtiene indices y extrae la nueva matriz\n",
    "indices = np.in1d(Y_completa, clases_retenidas)\n",
    "X = caras[indices]\n",
    "Y = Y_completa[indices]\n",
    "Y = Y.reshape(-1, 1) # Vector columna\n",
    "\n",
    "# Revuelve en forma aleatoria los datos\n",
    "indices = range(Y.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices,:]\n",
    "Y = Y[indices,:]\n",
    "\n",
    "# Separa el 25% de los datos como datos de prueba\n",
    "separa = 3 * Y.shape[0] / 4\n",
    "X_e, X_t = X[:separa,:], X[separa:,:]\n",
    "Y_e, Y_t = Y[:separa,:], Y[separa:,:]\n",
    "\n",
    "\n",
    "# Imprime la información sobre los datos retenidos\n",
    "# para aprendizaje y prueba \n",
    "\n",
    "print \"Las caras que se repitan mas son: \\n\"\n",
    "print caras_diferentes[clases_retenidas]\n",
    "print \"\\nTamano del conjunto de aprendizaje:\\n\"\n",
    "print \"Numero de atributos: \", X_e.shape[1]\n",
    "print \"Numero de elementos para aprendizaje: \", X_e.shape[0]\n",
    "print \"Numero de elementos para prueba: \", X_t.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Preprocesamiento de datos de entrada\n",
    "\n",
    "Si tomamos tal cual la información para clasificación,\n",
    "entonces tenemos 64 $\\times$ 64 atributos (1024), de los cuales\n",
    "muchos son redundantes.\n",
    "\n",
    "Para reconocimiento de rostros se utiliza muy normalmente\n",
    "una técnica conocida como *eigenfaces*, la cual no es más\n",
    "que aplicar una transformación lineal conocida como **Análisis\n",
    "en componentes principales** a la información que se obtiene\n",
    "en un vector que representa pixeles.\n",
    "\n",
    "El análisis en componentes principales (PCA), asume que los datos\n",
    "de aprendizaje son significativos estadísticamente, por lo que\n",
    "se le aplica una matriz de rotación a los datos con el fin de\n",
    "buscar un cambio de coordenandas, en las cuales las covarianzas\n",
    "entre cualquier par de dimensiones dimensiones diferentes sea 0. \n",
    "\n",
    "Para realizar el PCA hay que realizar una operación muy macabrona del\n",
    "análisis numérico: la obtención de valores propios y vectores propios.\n",
    "En general los algoritmos eficientes (numéricamente) solo funcionan\n",
    "para matrices densas y pequeñas o para matrices muy grandes y dispersas.\n",
    "\n",
    "``Scikit.learn`` provee tres métodos de PCA, donde solo varía la manera\n",
    "en que obtiene los valores propios y vectores propios. Para no fallar,\n",
    "utilizaremos ``RandomizedPCA`` que funciona tanto para matrices pequeñas y densas\n",
    "como dispersas, pero que es menos eficiente que los otros dos métodos que son\n",
    "especializados para cada caso.\n",
    "\n",
    "Es importante recordar que la extracción de características y\n",
    "normalización siempre se realiza asumiendo que los únicos datos\n",
    "conocidos son los de aprendizaje, y luego se transforman los datos\n",
    "de prueba a partir de los datos establecidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aqui se encuentra el método de componentes principales\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "# Número de componentes principales retenidas\n",
    "componentes_principales = 150\n",
    "\n",
    "# Genera un objeto pca con n_components componentes principales retenidas.\n",
    "# whiten=True implica un escalamiento de las componentes para asegurar\n",
    "# que sean completamente ortogonales entre ellas, aunque se pierde la posibilidad\n",
    "# de obtener de nuevo los valores originales a partir de las componentes principales.\n",
    "pca = RandomizedPCA(n_components=componentes_principales, whiten=True)\n",
    "\n",
    "# Ajusta las componentes con los datos de aprendizaje\n",
    "pca.fit(X_e)\n",
    "\n",
    "# Obtiene los datos en PCA para usar tanto para aprendizaje como prueba\n",
    "X_e_pca = pca.transform(X_e)\n",
    "X_t_pca = pca.transform(X_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Fácil, verdad? Ahora veamos que se obtuvo de esto. Recuerda que el número de componentes requeridas lo puedes variar (mientras menos componentes, menos información pero ménos parámetros necesarios en el aprendizaje, unas por otras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imprime la información sobre los datos preprocesados\n",
    "# así como la imagen de la <<eigenface>> o máscara que\n",
    "# permite encontrar los componentes más determinantes\n",
    "# para encontrar diferencias entre los datos. \n",
    "\n",
    "print \"\\nTamaño del conjunto de aprendizaje preprocesado:\\n\"\n",
    "print \"Atributos: \", X_e_pca.shape[1], \"\\t Elementos: \", X_e_pca.shape[0]\n",
    "\n",
    "# Se obtienen las \"Caras genéricas (una por componente principal)\"\n",
    "# solo para ver las caras más genérica por curiosidad\n",
    "eigenfaces = pca.components_.reshape((componentes_principales, 64, 64))\n",
    "\n",
    "columnas, renglones = 4, 3\n",
    "for i in range(columnas * renglones):\n",
    "    plt.subplot(renglones, columnas, i + 1)\n",
    "    plt.imshow(eigenfaces[i,:,:].copy().squeeze(), cmap=plt.cm.gray)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.suptitle(\"Las primeras 12 <eigenfaces>\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clasificación utilizando una Máquina de vectores de soporte lineal\n",
    "\n",
    "Vamos ahora a utilizar la máquina de vectes de soporte sin el truco del kernel, o lo que es lo mismo, con un kernel lineal. Para esto vamos a utilizar la máquina de vector de soporte que viene incluida en la librería de ``scikit-learn``. Es de tener en cuenta que para los clasificadores que solo usan Kernel lineal existe su propia función, la cual se encuentra optimizada, a diferencia que la función que utiliza el truco del kernel, la cual no se puede optimizar (en velocidad de ejecución tanto). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Esta es la clase de clasificadores tipo SVM con kernel lineal\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Ahora vamos a obtener las funciones para evaluar un clasificador\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es la mejor forma de ajustar un clasificador (y definitivamente no la vamos\n",
    "a usar mas adelanta así), pero vamos a ajustar los valores del clasificador a mano.\n",
    "\n",
    "Esto es, vamos a utilizar los datos de entrenamiento para aprender, los de prueba para estimarlos, y \n",
    "vamos a revisar con la función ``classification_report`` los indicadores básicos de un clasificador,\n",
    "por clase y globales.\n",
    "\n",
    "Esto implica que entonces vamos a manosear bastante los datos de prueba, por lo que serán en este caso, datos de\n",
    "validación más que de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Genera el objeto clasificador SVM lineal\n",
    "C = 1.0\n",
    "clasificador = LinearSVC(C=C)\n",
    "\n",
    "# Entrenamiento\n",
    "clasificador.fit(X_e_pca, Y_e.squeeze())\n",
    "\n",
    "#Predicción\n",
    "Y_estimada = clasificador.predict(X_t_pca)\n",
    "\n",
    "print \"El porcentaje de acierto, con C = \", C, \" es de \",\n",
    "print accuracy_score(Y_t.squeeze(), Y_estimada)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Analizando los PCA y el SVM lineal\n",
    "\n",
    "Responde a las siguientes preguntas. Para hacerlo utiliza las celdas siguientes. Una para poder realizar pruebas en python y corroborar las ideas que se quieran plasmar, y el otro donde se responde a cada una de las preguntas.\n",
    "\n",
    "**Preguntas**\n",
    "\n",
    "1. Ajusta el valor de C para obtener el mayor valor del porcentaje de acierto. ¿Cuál es el valor de C?\n",
    "   ¿Cual es el mejor porcentaje de acierto?\n",
    "\n",
    "2. ¿Se obtienen los mismos resultados si se ejecuta la libreta completa varias veces? ¿Porqué?\n",
    "\n",
    "3. ¿Que pasa con el error de clasificación si en lugar de usar las *eigenfaces* se utilizan los datos originales?\n",
    "\n",
    "4. ¿Que pasa si se usan solo 50 componentes principales? ¿Que pasa si se utilizan 500?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Escribe aqui el código necesario para responder a las preguntas anteriores\n",
    "\n",
    "# Para la pregunta 1\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 2\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 3\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuestas a las preguntas del ejercicio 1\n",
    "\n",
    "**Pregunta 1:**\n",
    "\n",
    "\n",
    "**Pregunta 2:**\n",
    "\n",
    "\n",
    "**Pregunta 3:**\n",
    "\n",
    "\n",
    "**Pregunta 4:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2. Analizando el uso de SVM con diferentes kernels\n",
    "\n",
    "Responde a las siguientes preguntas. Para hacerlo utiliza las celdas siguientes. Una para poder realizar pruebas en python y corroborar las ideas que se quieran plasmar, y el otro donde se responde a cada una de las preguntas.\n",
    "\n",
    "**Preguntas**\n",
    "\n",
    "1. Utiliza una SVM con kernel polinomial. Ajusta los valores de $C$ y el grado del polinomio para aumentar el porcentaje de acierto en los datos de validaciñon. ¿Cuales son los valores de $C$ y el grado del polinomio con mejores resultados?\n",
    "\n",
    "2. Utiliza una SVM con kernel gaussiano. Ajusta los valores de $C$ y $\\gamma$ para aumentar el porcentaje de acierto en los datos de validaciñon. ¿Cuales son los valores de $C$ y $\\gamma$ con mejores resultados?\n",
    "\n",
    "3. ¿Cual Kernel prefieres? ¿Porqué?\n",
    "\n",
    "4. ¿Que pasa con el error de clasificación si en lugar de usar las *eigenfaces* se utilizan los datos originales?\n",
    "\n",
    "5. ¿Que pasa si se usan solo 50 componentes principales? ¿Que pasa si se utilizan 500?\n",
    "\n",
    "6. ¿Porque crees que es bueno usar las eigenfaces?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Escribe aqui el código necesario para responder a las preguntas anteriores\n",
    "\n",
    "# Para la pregunta 1\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 2\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 3\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 4\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 5\n",
    "\n",
    "\n",
    "\n",
    "# Para la pregunta 6\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuestas a las preguntas del ejercicio 2\n",
    "\n",
    "**Pregunta 1:**\n",
    "\n",
    "\n",
    "**Pregunta 2:**\n",
    "\n",
    "\n",
    "**Pregunta 3:**\n",
    "\n",
    "\n",
    "**Pregunta 4:**\n",
    "\n",
    "\n",
    "**Pregunta 5:**\n",
    "\n",
    "\n",
    "**Pregunta 6:**\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
